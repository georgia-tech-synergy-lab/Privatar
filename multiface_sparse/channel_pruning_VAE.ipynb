{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verified Actual Channel Pruning Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.conv1.state_dict()['weight'])\n",
    "print(model.conv1.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_ratio = 0.3 \n",
    "\n",
    "def weights_kernel_pruning_l1_norm(model, prune_ratio):\n",
    "    layer_shape = model.state_dict()['weight'].size()\n",
    "    weight_copy = model.weight.data.abs().clone()\n",
    "    \n",
    "    l1_norm = torch.sum(weight_copy, dim=(1, 2, 3))\n",
    "    num_channels_to_prune = int(prune_ratio * layer_shape[0])\n",
    "    response_val, prune_indices = torch.topk(l1_norm, num_channels_to_prune, largest=False)\n",
    "    overall_indices = set([i for i in range(layer_shape[0])])\n",
    "    prune_indices = set(prune_indices.tolist())\n",
    "    remaining_indices = overall_indices - prune_indices\n",
    "\n",
    "    in_weights_float = torch.zeros((len(remaining_indices), int(layer_shape[1]), int(layer_shape[2]), int(layer_shape[3])), dtype=torch.float)\n",
    "    in_weights_float = weight_copy[list(remaining_indices),:,:,:]\n",
    "    model.weight = torch.nn.Parameter(in_weights_float)\n",
    "    print(f\"under prune_ratio={prune_ratio}, num_channels_to_prune={num_channels_to_prune}, response_val={response_val}, remaining_indices={remaining_indices}, prune_indices={prune_indices}\")\n",
    "    return model, prune_indices\n",
    "\n",
    "model.conv1, prune_indices = weights_kernel_pruning_l1_norm(model.conv1, prune_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.conv1\n",
    "print(list(module.named_parameters()))\n",
    "print(module.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iAct_channel_pruning_l1_norm(model, prune_indices):\n",
    "    layer_shape = model.state_dict()['weight'].size()\n",
    "    weight_copy = model.weight.data.abs().clone()\n",
    "    \n",
    "    prune_indices = set(prune_indices)\n",
    "    overall_indices = set([i for i in range(layer_shape[1])])\n",
    "    remaining_indices = overall_indices - prune_indices\n",
    "\n",
    "    in_weights_float = torch.zeros((int(layer_shape[0]), int(len(remaining_indices)), int(layer_shape[2]), int(layer_shape[3])), dtype=torch.float)\n",
    "    in_weights_float = weight_copy[:, list(remaining_indices), :, :]\n",
    "    model.weight = torch.nn.Parameter(in_weights_float)\n",
    "    print(f\"prune input channel indice={prune_indices}, num_channels_to_prune={len(prune_indices)}, remaining_indices={remaining_indices}, prune_indices={prune_indices}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.conv2.weight.shape)\n",
    "model.conv2 = iAct_channel_pruning_l1_norm(model.conv2, prune_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.conv2\n",
    "print(list(module.named_parameters()))\n",
    "print(module.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Overall Pruning -- Inference only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianming/conda/envs/pica37/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from dataset import Dataset\n",
    "from models import DeepAppearanceVAE, WarpFieldVAE, ConvTranspose2dWN\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from utils import Renderer, gammaCorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_kernel_pruning_l1_norm(model, in_bias, prune_ratio):\n",
    "    layer_shape = model.state_dict()['weight'].size()\n",
    "    weight_copy = model.weight.data.abs().clone()\n",
    "    \n",
    "    l1_norm = torch.sum(weight_copy, dim=(0, 2, 3))\n",
    "    num_channels_to_prune = int(prune_ratio * layer_shape[1])\n",
    "    response_val, prune_indices = torch.topk(l1_norm, num_channels_to_prune, largest=False)\n",
    "    overall_indices = set([i for i in range(layer_shape[1])])\n",
    "    prune_indices = set(prune_indices.tolist())\n",
    "    remaining_indices = overall_indices - prune_indices\n",
    "\n",
    "    new_model = ConvTranspose2dWN(int(layer_shape[0]), int(len(remaining_indices)), kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False)\n",
    "    print(f\"remaining_indices={remaining_indices}\")\n",
    "    print(f\"weight kernel pruning original bias dimension = {in_bias.shape}\")\n",
    "    out_bias = torch.nn.Parameter(in_bias[:,list(remaining_indices),:,:])\n",
    "\n",
    "    in_weights_float = torch.zeros((int(layer_shape[0]), len(remaining_indices), int(layer_shape[2]), int(layer_shape[3])), dtype=torch.float)\n",
    "    in_weights_float = weight_copy[:, list(remaining_indices), :, :]\n",
    "    new_model.weight = torch.nn.Parameter(in_weights_float)\n",
    "    print(f\"under prune_ratio={prune_ratio}, num_channels_to_prune={num_channels_to_prune}, response_val={response_val}, remaining_indices={remaining_indices}, prune_indices={prune_indices}\")\n",
    "    return new_model, out_bias, prune_indices\n",
    "\n",
    "def iAct_channel_pruning_l1_norm(model, prune_indices):\n",
    "    layer_shape = model.state_dict()['weight'].size()\n",
    "    weight_copy = model.weight.data.abs().clone()\n",
    "    \n",
    "    prune_indices = set(prune_indices)\n",
    "    overall_indices = set([i for i in range(layer_shape[0])])\n",
    "    remaining_indices = overall_indices - prune_indices\n",
    "\n",
    "    new_model = ConvTranspose2dWN(len(remaining_indices), int(layer_shape[1]), kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False)\n",
    "    \n",
    "    in_weights_float = torch.zeros((int(len(remaining_indices)), int(layer_shape[1]), int(layer_shape[2]), int(layer_shape[3])), dtype=torch.float)\n",
    "    in_weights_float = weight_copy[list(remaining_indices), :, :, :]\n",
    "    new_model.weight = torch.nn.Parameter(in_weights_float)\n",
    "    print(f\"prune input channel indice={prune_indices}, num_channels_to_prune={len(prune_indices)}, remaining_indices={remaining_indices}, prune_indices={prune_indices}\")\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepAppearanceVAE(1024, 21918, n_latent=256, n_cams=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256, 4, 4])\n",
      "torch.Size([256, 64, 4, 4])\n",
      "torch.Size([64, 64, 4, 4])\n",
      "torch.Size([64, 32, 4, 4])\n",
      "torch.Size([32, 32, 4, 4])\n",
      "torch.Size([32, 16, 4, 4])\n",
      "torch.Size([16, 16, 4, 4])\n",
      "torch.Size([16, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(model.dec.texture_decoder.upsample[0].conv1.deconv.weight.shape)\n",
    "print(model.dec.texture_decoder.upsample[0].conv2.deconv.weight.shape)\n",
    "\n",
    "print(model.dec.texture_decoder.upsample[1].conv1.deconv.weight.shape)\n",
    "print(model.dec.texture_decoder.upsample[1].conv2.deconv.weight.shape)\n",
    "\n",
    "print(model.dec.texture_decoder.upsample[2].conv1.deconv.weight.shape)\n",
    "print(model.dec.texture_decoder.upsample[2].conv2.deconv.weight.shape)\n",
    "\n",
    "print(model.dec.texture_decoder.upsample[3].conv1.deconv.weight.shape)\n",
    "print(model.dec.texture_decoder.upsample[3].conv2.deconv.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remaining_indices={0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 13, 15, 17, 18, 20, 22, 23, 24, 26, 27, 29, 30, 31, 32, 33, 34, 35, 37, 38, 40, 42, 44, 45, 47, 48, 52, 53, 54, 55, 56, 57, 59, 62, 66, 68, 69, 70, 71, 72, 73, 74, 77, 78, 80, 83, 84, 86, 88, 89, 90, 91, 95, 96, 97, 98, 100, 101, 102, 104, 105, 108, 109, 110, 112, 113, 115, 116, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 131, 133, 134, 135, 136, 137, 138, 140, 142, 143, 145, 147, 148, 149, 150, 151, 152, 154, 157, 158, 159, 162, 163, 164, 165, 166, 168, 169, 170, 174, 175, 176, 177, 178, 179, 180, 181, 182, 185, 186, 187, 188, 189, 191, 192, 193, 195, 197, 198, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 216, 217, 219, 220, 222, 223, 224, 226, 227, 228, 229, 231, 232, 234, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255}\n",
      "weight kernel pruning original bias dimension = torch.Size([1, 256, 8, 8])\n",
      "under prune_ratio=0.3, num_channels_to_prune=76, response_val=tensor([145.6904, 146.1420, 146.5318, 146.9443, 146.9935, 147.3919, 147.7506,\n",
      "        148.1224, 148.4870, 148.5659, 148.9393, 149.1379, 149.1408, 149.1917,\n",
      "        149.3245, 149.4113, 149.5724, 149.5898, 149.5949, 149.6017, 149.7613,\n",
      "        149.7639, 149.8202, 149.9127, 149.9614, 149.9731, 149.9794, 150.0472,\n",
      "        150.0926, 150.1360, 150.1534, 150.1570, 150.1580, 150.3838, 150.3900,\n",
      "        150.4157, 150.4596, 150.6605, 150.7875, 150.7927, 150.8151, 150.9588,\n",
      "        150.9619, 150.9891, 150.9926, 151.0113, 151.0428, 151.1024, 151.1485,\n",
      "        151.1487, 151.1926, 151.2352, 151.3299, 151.4933, 151.5533, 151.6228,\n",
      "        151.6499, 151.6825, 151.7010, 151.7203, 151.8170, 151.8722, 151.9081,\n",
      "        151.9473, 152.0226, 152.0292, 152.0878, 152.1025, 152.1196, 152.1245,\n",
      "        152.1340, 152.1558, 152.1817, 152.1892, 152.2462, 152.2716]), remaining_indices={0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 13, 15, 17, 18, 20, 22, 23, 24, 26, 27, 29, 30, 31, 32, 33, 34, 35, 37, 38, 40, 42, 44, 45, 47, 48, 52, 53, 54, 55, 56, 57, 59, 62, 66, 68, 69, 70, 71, 72, 73, 74, 77, 78, 80, 83, 84, 86, 88, 89, 90, 91, 95, 96, 97, 98, 100, 101, 102, 104, 105, 108, 109, 110, 112, 113, 115, 116, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 131, 133, 134, 135, 136, 137, 138, 140, 142, 143, 145, 147, 148, 149, 150, 151, 152, 154, 157, 158, 159, 162, 163, 164, 165, 166, 168, 169, 170, 174, 175, 176, 177, 178, 179, 180, 181, 182, 185, 186, 187, 188, 189, 191, 192, 193, 195, 197, 198, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 216, 217, 219, 220, 222, 223, 224, 226, 227, 228, 229, 231, 232, 234, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255}, prune_indices={129, 130, 2, 132, 5, 139, 12, 141, 14, 144, 16, 146, 19, 21, 25, 153, 155, 28, 156, 160, 161, 36, 39, 167, 41, 171, 172, 43, 173, 46, 49, 50, 51, 183, 184, 58, 60, 61, 190, 63, 64, 65, 194, 67, 196, 200, 201, 75, 76, 79, 81, 82, 211, 85, 87, 215, 218, 92, 221, 94, 93, 225, 99, 230, 103, 233, 106, 235, 107, 111, 239, 114, 117, 120, 252, 253}\n",
      "prune input channel indice={2, 5, 12, 14, 16, 19, 21, 25, 28, 36, 39, 41, 43, 46, 49, 50, 51, 58, 60, 61, 63, 64, 65, 67, 75, 76, 79, 81, 82, 85, 87, 92, 93, 94, 99, 103, 106, 107, 111, 114, 117, 120, 129, 130, 132, 139, 141, 144, 146, 153, 155, 156, 160, 161, 167, 171, 172, 173, 183, 184, 190, 194, 196, 200, 201, 211, 215, 218, 221, 225, 230, 233, 235, 239, 252, 253}, num_channels_to_prune=76, remaining_indices={0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 13, 15, 17, 18, 20, 22, 23, 24, 26, 27, 29, 30, 31, 32, 33, 34, 35, 37, 38, 40, 42, 44, 45, 47, 48, 52, 53, 54, 55, 56, 57, 59, 62, 66, 68, 69, 70, 71, 72, 73, 74, 77, 78, 80, 83, 84, 86, 88, 89, 90, 91, 95, 96, 97, 98, 100, 101, 102, 104, 105, 108, 109, 110, 112, 113, 115, 116, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 131, 133, 134, 135, 136, 137, 138, 140, 142, 143, 145, 147, 148, 149, 150, 151, 152, 154, 157, 158, 159, 162, 163, 164, 165, 166, 168, 169, 170, 174, 175, 176, 177, 178, 179, 180, 181, 182, 185, 186, 187, 188, 189, 191, 192, 193, 195, 197, 198, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 216, 217, 219, 220, 222, 223, 224, 226, 227, 228, 229, 231, 232, 234, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 254, 255}, prune_indices={2, 5, 12, 14, 16, 19, 21, 25, 28, 36, 39, 41, 43, 46, 49, 50, 51, 58, 60, 61, 63, 64, 65, 67, 75, 76, 79, 81, 82, 85, 87, 92, 93, 94, 99, 103, 106, 107, 111, 114, 117, 120, 129, 130, 132, 139, 141, 144, 146, 153, 155, 156, 160, 161, 167, 171, 172, 173, 183, 184, 190, 194, 196, 200, 201, 211, 215, 218, 221, 225, 230, 233, 235, 239, 252, 253}\n",
      "remaining_indices={0, 1, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 27, 28, 31, 32, 33, 34, 36, 38, 39, 40, 41, 42, 45, 46, 48, 49, 51, 53, 54, 55, 59, 61, 62, 63}\n",
      "weight kernel pruning original bias dimension = torch.Size([1, 64, 16, 16])\n",
      "under prune_ratio=0.3, num_channels_to_prune=19, response_val=tensor([127.9641, 130.2171, 131.8812, 132.3068, 132.7817, 132.9449, 133.5320,\n",
      "        133.6174, 133.7137, 134.0199, 134.0713, 134.1074, 134.2204, 134.2617,\n",
      "        134.4930, 134.5444, 134.7660, 134.9581, 135.0403]), remaining_indices={0, 1, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 27, 28, 31, 32, 33, 34, 36, 38, 39, 40, 41, 42, 45, 46, 48, 49, 51, 53, 54, 55, 59, 61, 62, 63}, prune_indices={2, 3, 9, 10, 24, 26, 29, 30, 35, 37, 43, 44, 47, 50, 52, 56, 57, 58, 60}\n",
      "prune input channel indice={2, 3, 9, 10, 24, 26, 29, 30, 35, 37, 43, 44, 47, 50, 52, 56, 57, 58, 60}, num_channels_to_prune=19, remaining_indices={0, 1, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 27, 28, 31, 32, 33, 34, 36, 38, 39, 40, 41, 42, 45, 46, 48, 49, 51, 53, 54, 55, 59, 61, 62, 63}, prune_indices={2, 3, 9, 10, 24, 26, 29, 30, 35, 37, 43, 44, 47, 50, 52, 56, 57, 58, 60}\n",
      "remaining_indices={0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 20, 21, 22, 24, 26, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 42, 43, 46, 48, 49, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 63}\n",
      "weight kernel pruning original bias dimension = torch.Size([1, 64, 32, 32])\n",
      "under prune_ratio=0.3, num_channels_to_prune=19, response_val=tensor([48.7441, 49.0478, 50.2524, 50.6891, 50.7568, 50.7998, 50.8426, 50.9580,\n",
      "        51.1230, 51.4064, 51.4471, 51.5832, 51.6192, 51.9414, 52.0474, 52.3033,\n",
      "        52.4070, 52.4392, 52.5016]), remaining_indices={0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 20, 21, 22, 24, 26, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 42, 43, 46, 48, 49, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 63}, prune_indices={4, 11, 12, 17, 18, 19, 23, 25, 27, 30, 39, 40, 41, 44, 45, 47, 50, 53, 62}\n",
      "prune input channel indice={4, 11, 12, 17, 18, 19, 23, 25, 27, 30, 39, 40, 41, 44, 45, 47, 50, 53, 62}, num_channels_to_prune=19, remaining_indices={0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 20, 21, 22, 24, 26, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 42, 43, 46, 48, 49, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 63}, prune_indices={4, 11, 12, 17, 18, 19, 23, 25, 27, 30, 39, 40, 41, 44, 45, 47, 50, 53, 62}\n",
      "remaining_indices={0, 1, 2, 3, 4, 6, 7, 9, 10, 11, 12, 13, 14, 15, 18, 19, 22, 23, 24, 25, 26, 30, 31}\n",
      "weight kernel pruning original bias dimension = torch.Size([1, 32, 64, 64])\n",
      "under prune_ratio=0.3, num_channels_to_prune=9, response_val=tensor([57.6786, 58.1344, 58.8287, 58.9633, 59.0474, 60.5278, 60.6267, 60.8480,\n",
      "        60.9573]), remaining_indices={0, 1, 2, 3, 4, 6, 7, 9, 10, 11, 12, 13, 14, 15, 18, 19, 22, 23, 24, 25, 26, 30, 31}, prune_indices={5, 8, 16, 17, 20, 21, 27, 28, 29}\n",
      "prune input channel indice={5, 8, 16, 17, 20, 21, 27, 28, 29}, num_channels_to_prune=9, remaining_indices={0, 1, 2, 3, 4, 6, 7, 9, 10, 11, 12, 13, 14, 15, 18, 19, 22, 23, 24, 25, 26, 30, 31}, prune_indices={5, 8, 16, 17, 20, 21, 27, 28, 29}\n",
      "remaining_indices={0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 15, 17, 18, 20, 21, 22, 24, 25, 27, 28, 29}\n",
      "weight kernel pruning original bias dimension = torch.Size([1, 32, 128, 128])\n",
      "under prune_ratio=0.3, num_channels_to_prune=9, response_val=tensor([33.1847, 34.3427, 35.5076, 35.6287, 36.3594, 36.5202, 37.1189, 37.2196,\n",
      "        37.3356]), remaining_indices={0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 15, 17, 18, 20, 21, 22, 24, 25, 27, 28, 29}, prune_indices={5, 7, 14, 16, 19, 23, 26, 30, 31}\n",
      "prune input channel indice={5, 7, 14, 16, 19, 23, 26, 30, 31}, num_channels_to_prune=9, remaining_indices={0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 15, 17, 18, 20, 21, 22, 24, 25, 27, 28, 29}, prune_indices={5, 7, 14, 16, 19, 23, 26, 30, 31}\n",
      "remaining_indices={0, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15}\n",
      "weight kernel pruning original bias dimension = torch.Size([1, 16, 256, 256])\n",
      "under prune_ratio=0.3, num_channels_to_prune=4, response_val=tensor([39.7584, 41.6079, 41.6249, 42.0470]), remaining_indices={0, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15}, prune_indices={1, 2, 11, 12}\n",
      "prune input channel indice={1, 2, 11, 12}, num_channels_to_prune=4, remaining_indices={0, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15}, prune_indices={1, 2, 11, 12}\n",
      "remaining_indices={0, 1, 2, 4, 6, 7, 9, 10, 11, 12, 14, 15}\n",
      "weight kernel pruning original bias dimension = torch.Size([1, 16, 512, 512])\n",
      "under prune_ratio=0.3, num_channels_to_prune=4, response_val=tensor([23.0485, 23.1752, 23.5731, 25.4172]), remaining_indices={0, 1, 2, 4, 6, 7, 9, 10, 11, 12, 14, 15}, prune_indices={8, 5, 3, 13}\n",
      "prune input channel indice={8, 5, 3, 13}, num_channels_to_prune=4, remaining_indices={0, 1, 2, 4, 6, 7, 9, 10, 11, 12, 14, 15}, prune_indices={8, 5, 3, 13}\n",
      "remaining_indices={0, 1, 2}\n",
      "weight kernel pruning original bias dimension = torch.Size([1, 3, 1024, 1024])\n",
      "under prune_ratio=0.3, num_channels_to_prune=0, response_val=tensor([]), remaining_indices={0, 1, 2}, prune_indices=set()\n"
     ]
    }
   ],
   "source": [
    "def model_decoder_pruning(model, unified_pruning_ratio):\n",
    "    model.dec.texture_decoder.upsample[0].conv1.deconv, model.dec.texture_decoder.upsample[0].conv1.bias, prune_indices_1 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[0].conv1.deconv, model.dec.texture_decoder.upsample[0].conv1.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[0].conv2.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[0].conv2.deconv, prune_indices_1)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[0].conv2.deconv, model.dec.texture_decoder.upsample[0].conv2.bias, prune_indices_2 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[0].conv2.deconv, model.dec.texture_decoder.upsample[0].conv2.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[1].conv1.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[1].conv1.deconv, prune_indices_2)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[1].conv1.deconv,  model.dec.texture_decoder.upsample[1].conv1.bias, prune_indices_3 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[1].conv1.deconv, model.dec.texture_decoder.upsample[1].conv1.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[1].conv2.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[1].conv2.deconv, prune_indices_3)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[1].conv2.deconv,  model.dec.texture_decoder.upsample[1].conv2.bias, prune_indices_4 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[1].conv2.deconv, model.dec.texture_decoder.upsample[1].conv2.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[2].conv1.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[2].conv1.deconv, prune_indices_4)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[2].conv1.deconv,  model.dec.texture_decoder.upsample[2].conv1.bias, prune_indices_5 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[2].conv1.deconv, model.dec.texture_decoder.upsample[2].conv1.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[2].conv2.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[2].conv2.deconv, prune_indices_5)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[2].conv2.deconv,  model.dec.texture_decoder.upsample[2].conv2.bias, prune_indices_6 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[2].conv2.deconv, model.dec.texture_decoder.upsample[2].conv2.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[3].conv1.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[3].conv1.deconv, prune_indices_6)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[3].conv1.deconv, model.dec.texture_decoder.upsample[3].conv1.bias, prune_indices_7 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[3].conv1.deconv, model.dec.texture_decoder.upsample[3].conv1.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[3].conv2.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[3].conv2.deconv, prune_indices_7)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[3].conv2.deconv,  model.dec.texture_decoder.upsample[3].conv2.bias, prune_indices_8 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[3].conv2.deconv, model.dec.texture_decoder.upsample[3].conv2.bias, unified_pruning_ratio)\n",
    "\n",
    "unified_pruning_ratio = 0.3\n",
    "model_decoder_pruning(unified_pruning_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 180, 4, 4])\n",
      "torch.Size([180, 45, 4, 4])\n",
      "torch.Size([45, 45, 4, 4])\n",
      "torch.Size([45, 23, 4, 4])\n",
      "torch.Size([23, 23, 4, 4])\n",
      "torch.Size([23, 12, 4, 4])\n",
      "torch.Size([12, 12, 4, 4])\n",
      "torch.Size([12, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(model.dec.texture_decoder.upsample[0].conv1.deconv.weight.shape)\n",
    "print(model.dec.texture_decoder.upsample[0].conv2.deconv.weight.shape)\n",
    "\n",
    "print(model.dec.texture_decoder.upsample[1].conv1.deconv.weight.shape)\n",
    "print(model.dec.texture_decoder.upsample[1].conv2.deconv.weight.shape)\n",
    "\n",
    "print(model.dec.texture_decoder.upsample[2].conv1.deconv.weight.shape)\n",
    "print(model.dec.texture_decoder.upsample[2].conv2.deconv.weight.shape)\n",
    "\n",
    "print(model.dec.texture_decoder.upsample[3].conv1.deconv.weight.shape)\n",
    "print(model.dec.texture_decoder.upsample[3].conv2.deconv.weight.shape)\n",
    "\n",
    "# Do not prune the last layer as the input images come with 3 channels.\n",
    "# model.dec.texture_decoder.upsample[3].conv2.deconv, prune_indices_8 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[3].conv2.deconv, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 1024, 1024])\n",
      "torch.Size([16, 7306, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "in_avg_tex = torch.randn([16, 3, 1024, 1024])\n",
    "in_verts = torch.randn([16, 7306, 3])\n",
    "in_view = torch.randn([16, 3])\n",
    "in_cams = torch.tensor([22, 37, 14, 19,  7, 11, 31,  2, 20, 20, 14, 21,  9,  6, 10,  5])\n",
    "\n",
    "print(in_avg_tex.shape)\n",
    "print(in_verts.shape)\n",
    "print(in_view.shape)\n",
    "print(in_cams.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape=torch.Size([16, 180, 8, 8])\n",
      "self.bias.shape=torch.Size([1, 180, 8, 8])\n",
      "out.shape=torch.Size([16, 45, 16, 16])\n",
      "self.bias.shape=torch.Size([1, 45, 16, 16])\n",
      "out.shape=torch.Size([16, 45, 32, 32])\n",
      "self.bias.shape=torch.Size([1, 45, 32, 32])\n",
      "out.shape=torch.Size([16, 23, 64, 64])\n",
      "self.bias.shape=torch.Size([1, 23, 64, 64])\n",
      "out.shape=torch.Size([16, 23, 128, 128])\n",
      "self.bias.shape=torch.Size([1, 23, 128, 128])\n",
      "out.shape=torch.Size([16, 12, 256, 256])\n",
      "self.bias.shape=torch.Size([1, 12, 256, 256])\n",
      "out.shape=torch.Size([16, 12, 512, 512])\n",
      "self.bias.shape=torch.Size([1, 12, 512, 512])\n",
      "out.shape=torch.Size([16, 3, 1024, 1024])\n",
      "self.bias.shape=torch.Size([1, 3, 1024, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.0536,  0.0732,  0.0732,  ...,  0.0738,  0.0738,  0.0606],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0719],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0719],\n",
       "           ...,\n",
       "           [ 0.0717,  0.1058,  0.1058,  ...,  0.1055,  0.1055,  0.0712],\n",
       "           [ 0.0717,  0.1058,  0.1058,  ...,  0.1055,  0.1055,  0.0712],\n",
       "           [ 0.0588,  0.0697,  0.0697,  ...,  0.0693,  0.0693,  0.0516]],\n",
       " \n",
       "          [[-0.1157, -0.1001, -0.1001,  ..., -0.1002, -0.1002, -0.1146],\n",
       "           [-0.1005, -0.0673, -0.0673,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           [-0.1005, -0.0673, -0.0673,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           ...,\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0674, -0.0674, -0.1001],\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0674, -0.0674, -0.1001],\n",
       "           [-0.1149, -0.1005, -0.1005,  ..., -0.1003, -0.1003, -0.1154]],\n",
       " \n",
       "          [[-0.0856, -0.0765, -0.0765,  ..., -0.0765, -0.0765, -0.0831],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           ...,\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0470, -0.0470, -0.0715],\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0470, -0.0470, -0.0715],\n",
       "           [-0.0777, -0.0661, -0.0661,  ..., -0.0660, -0.0660, -0.0809]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0536,  0.0732,  0.0732,  ...,  0.0738,  0.0738,  0.0606],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0719],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0719],\n",
       "           ...,\n",
       "           [ 0.0717,  0.1058,  0.1058,  ...,  0.1056,  0.1056,  0.0712],\n",
       "           [ 0.0717,  0.1058,  0.1058,  ...,  0.1056,  0.1056,  0.0712],\n",
       "           [ 0.0588,  0.0697,  0.0697,  ...,  0.0693,  0.0693,  0.0516]],\n",
       " \n",
       "          [[-0.1157, -0.1001, -0.1001,  ..., -0.1002, -0.1002, -0.1146],\n",
       "           [-0.1005, -0.0673, -0.0673,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           [-0.1005, -0.0673, -0.0673,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           ...,\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0674, -0.0674, -0.1001],\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0674, -0.0674, -0.1001],\n",
       "           [-0.1149, -0.1005, -0.1005,  ..., -0.1003, -0.1003, -0.1154]],\n",
       " \n",
       "          [[-0.0856, -0.0765, -0.0765,  ..., -0.0765, -0.0765, -0.0831],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           ...,\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0470, -0.0470, -0.0715],\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0470, -0.0470, -0.0715],\n",
       "           [-0.0777, -0.0661, -0.0661,  ..., -0.0660, -0.0660, -0.0809]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0536,  0.0732,  0.0732,  ...,  0.0738,  0.0738,  0.0606],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0719],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0719],\n",
       "           ...,\n",
       "           [ 0.0717,  0.1058,  0.1058,  ...,  0.1055,  0.1055,  0.0712],\n",
       "           [ 0.0717,  0.1058,  0.1058,  ...,  0.1055,  0.1055,  0.0712],\n",
       "           [ 0.0588,  0.0697,  0.0697,  ...,  0.0693,  0.0693,  0.0516]],\n",
       " \n",
       "          [[-0.1157, -0.1001, -0.1001,  ..., -0.1002, -0.1002, -0.1146],\n",
       "           [-0.1005, -0.0673, -0.0673,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           [-0.1005, -0.0673, -0.0673,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           ...,\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0674, -0.0674, -0.1001],\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0674, -0.0674, -0.1001],\n",
       "           [-0.1149, -0.1005, -0.1005,  ..., -0.1003, -0.1003, -0.1154]],\n",
       " \n",
       "          [[-0.0856, -0.0765, -0.0765,  ..., -0.0765, -0.0765, -0.0831],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           ...,\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0470, -0.0470, -0.0715],\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0470, -0.0470, -0.0715],\n",
       "           [-0.0777, -0.0661, -0.0661,  ..., -0.0660, -0.0660, -0.0809]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.0536,  0.0732,  0.0732,  ...,  0.0738,  0.0738,  0.0606],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0718],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0718],\n",
       "           ...,\n",
       "           [ 0.0717,  0.1058,  0.1058,  ...,  0.1055,  0.1055,  0.0712],\n",
       "           [ 0.0717,  0.1058,  0.1058,  ...,  0.1055,  0.1055,  0.0712],\n",
       "           [ 0.0588,  0.0697,  0.0697,  ...,  0.0693,  0.0693,  0.0516]],\n",
       " \n",
       "          [[-0.1157, -0.1001, -0.1001,  ..., -0.1002, -0.1002, -0.1146],\n",
       "           [-0.1005, -0.0674, -0.0674,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           [-0.1005, -0.0674, -0.0674,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           ...,\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0675, -0.0675, -0.1001],\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0675, -0.0675, -0.1001],\n",
       "           [-0.1149, -0.1005, -0.1005,  ..., -0.1003, -0.1003, -0.1154]],\n",
       " \n",
       "          [[-0.0856, -0.0765, -0.0765,  ..., -0.0765, -0.0765, -0.0831],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           ...,\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0471, -0.0471, -0.0715],\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0471, -0.0471, -0.0715],\n",
       "           [-0.0777, -0.0661, -0.0661,  ..., -0.0660, -0.0660, -0.0809]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0536,  0.0732,  0.0732,  ...,  0.0738,  0.0738,  0.0606],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0719],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0719],\n",
       "           ...,\n",
       "           [ 0.0717,  0.1059,  0.1059,  ...,  0.1056,  0.1056,  0.0712],\n",
       "           [ 0.0717,  0.1059,  0.1059,  ...,  0.1056,  0.1056,  0.0712],\n",
       "           [ 0.0588,  0.0697,  0.0697,  ...,  0.0693,  0.0693,  0.0516]],\n",
       " \n",
       "          [[-0.1157, -0.1001, -0.1001,  ..., -0.1002, -0.1002, -0.1146],\n",
       "           [-0.1005, -0.0673, -0.0673,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           [-0.1005, -0.0673, -0.0673,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           ...,\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0674, -0.0674, -0.1001],\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0674, -0.0674, -0.1001],\n",
       "           [-0.1149, -0.1005, -0.1005,  ..., -0.1003, -0.1003, -0.1154]],\n",
       " \n",
       "          [[-0.0856, -0.0765, -0.0765,  ..., -0.0765, -0.0765, -0.0831],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           ...,\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0470, -0.0470, -0.0715],\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0470, -0.0470, -0.0715],\n",
       "           [-0.0777, -0.0661, -0.0661,  ..., -0.0660, -0.0660, -0.0809]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0536,  0.0732,  0.0732,  ...,  0.0738,  0.0738,  0.0606],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0718],\n",
       "           [ 0.0714,  0.1056,  0.1056,  ...,  0.1068,  0.1068,  0.0718],\n",
       "           ...,\n",
       "           [ 0.0717,  0.1058,  0.1058,  ...,  0.1055,  0.1055,  0.0712],\n",
       "           [ 0.0717,  0.1058,  0.1058,  ...,  0.1055,  0.1055,  0.0712],\n",
       "           [ 0.0588,  0.0697,  0.0697,  ...,  0.0693,  0.0693,  0.0516]],\n",
       " \n",
       "          [[-0.1157, -0.1001, -0.1001,  ..., -0.1002, -0.1002, -0.1146],\n",
       "           [-0.1005, -0.0673, -0.0673,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           [-0.1005, -0.0673, -0.0673,  ..., -0.0675, -0.0675, -0.1003],\n",
       "           ...,\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0674, -0.0674, -0.1001],\n",
       "           [-0.1007, -0.0676, -0.0676,  ..., -0.0674, -0.0674, -0.1001],\n",
       "           [-0.1149, -0.1005, -0.1005,  ..., -0.1003, -0.1003, -0.1154]],\n",
       " \n",
       "          [[-0.0856, -0.0765, -0.0765,  ..., -0.0765, -0.0765, -0.0831],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           [-0.0712, -0.0483, -0.0483,  ..., -0.0481, -0.0481, -0.0718],\n",
       "           ...,\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0470, -0.0470, -0.0715],\n",
       "           [-0.0709, -0.0471, -0.0471,  ..., -0.0470, -0.0470, -0.0715],\n",
       "           [-0.0777, -0.0661, -0.0661,  ..., -0.0660, -0.0660, -0.0809]]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[-7.6090e-02, -4.0847e-02, -1.0565e-01],\n",
       "          [ 1.9447e-01, -1.2061e-01, -2.5843e-01],\n",
       "          [-3.1237e-01,  3.5770e-01, -1.1043e-01],\n",
       "          ...,\n",
       "          [-2.2444e-02, -1.0658e-01,  1.2430e-01],\n",
       "          [-2.7563e-01,  5.2824e-01, -1.5545e-01],\n",
       "          [-4.5172e-02,  1.7424e-01,  1.4057e-02]],\n",
       " \n",
       "         [[-1.8477e-01,  1.4126e-01,  7.8379e-02],\n",
       "          [ 6.5105e-02, -3.7958e-02, -1.6226e-02],\n",
       "          [-2.0892e-01,  8.1364e-02,  1.8214e-01],\n",
       "          ...,\n",
       "          [ 1.8921e-01, -1.8000e-01,  5.8691e-02],\n",
       "          [ 6.7610e-02,  4.7924e-03,  1.0065e-01],\n",
       "          [-1.0045e-01,  4.1642e-02,  2.6984e-03]],\n",
       " \n",
       "         [[ 5.9728e-02,  1.3838e-01, -9.5308e-02],\n",
       "          [ 1.4896e-01, -1.3802e-01, -4.8400e-02],\n",
       "          [-5.4146e-02,  2.1899e-01,  1.3489e-01],\n",
       "          ...,\n",
       "          [ 1.4878e-01,  4.0255e-02,  3.7774e-02],\n",
       "          [-1.5510e-02,  2.5374e-02, -1.1713e-02],\n",
       "          [-8.9939e-02,  1.2523e-01, -3.6427e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.1218e-01, -4.6566e-02,  2.4977e-04],\n",
       "          [ 6.8770e-02, -1.5272e-01, -3.8025e-02],\n",
       "          [-2.0647e-01, -1.6179e-01,  1.2646e-01],\n",
       "          ...,\n",
       "          [ 1.4854e-01, -7.9999e-03,  1.7614e-01],\n",
       "          [-2.4510e-01,  1.1345e-01,  8.3803e-02],\n",
       "          [ 1.7687e-01,  3.7304e-02, -2.7555e-01]],\n",
       " \n",
       "         [[-3.2738e-01,  4.1987e-02,  8.6580e-03],\n",
       "          [ 6.9775e-02, -4.6454e-02,  1.9073e-01],\n",
       "          [-6.6157e-01,  5.5414e-02, -8.2051e-02],\n",
       "          ...,\n",
       "          [-3.1175e-02, -7.3193e-02, -4.6291e-02],\n",
       "          [ 1.2995e-01,  2.4539e-02,  4.3684e-02],\n",
       "          [-2.0709e-01, -4.7063e-02,  1.2929e-01]],\n",
       " \n",
       "         [[-6.9139e-02, -5.8914e-02, -5.6901e-02],\n",
       "          [ 2.4925e-01, -1.3983e-01, -1.2532e-01],\n",
       "          [ 1.1961e-01,  6.3017e-02,  1.6538e-01],\n",
       "          ...,\n",
       "          [ 1.4634e-01, -1.8026e-01,  4.0031e-01],\n",
       "          [ 1.1799e-01, -1.4968e-02,  4.7692e-02],\n",
       "          [ 4.0549e-01, -1.3248e-01, -2.2742e-01]]], grad_fn=<ViewBackward0>),\n",
       " tensor(0.0057, grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(in_avg_tex, in_verts, in_view, in_cams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Overall Pruning -- Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from dataset import Dataset\n",
    "from models import DeepAppearanceVAE, WarpFieldVAE, ConvTranspose2dWN\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from utils import Renderer, gammaCorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_kernel_pruning_l1_norm(model, in_bias, prune_ratio):\n",
    "    layer_shape = model.state_dict()['weight'].size()\n",
    "    weight_copy = model.weight.data.abs().clone()\n",
    "    \n",
    "    l1_norm = torch.sum(weight_copy, dim=(0, 2, 3))\n",
    "    num_channels_to_prune = int(prune_ratio * layer_shape[1])\n",
    "    response_val, prune_indices = torch.topk(l1_norm, num_channels_to_prune, largest=False)\n",
    "    overall_indices = set([i for i in range(layer_shape[1])])\n",
    "    prune_indices = set(prune_indices.tolist())\n",
    "    remaining_indices = overall_indices - prune_indices\n",
    "\n",
    "    new_model = ConvTranspose2dWN(int(layer_shape[0]), int(len(remaining_indices)), kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False)\n",
    "    print(f\"remaining_indices={remaining_indices}\")\n",
    "    print(f\"weight kernel pruning original bias dimension = {in_bias.shape}\")\n",
    "    out_bias = torch.nn.Parameter(in_bias[:,list(remaining_indices),:,:])\n",
    "\n",
    "    in_weights_float = torch.zeros((int(layer_shape[0]), len(remaining_indices), int(layer_shape[2]), int(layer_shape[3])), dtype=torch.float)\n",
    "    in_weights_float = weight_copy[:, list(remaining_indices), :, :]\n",
    "    new_model.weight = torch.nn.Parameter(in_weights_float)\n",
    "    print(f\"under prune_ratio={prune_ratio}, num_channels_to_prune={num_channels_to_prune}, response_val={response_val}, remaining_indices={remaining_indices}, prune_indices={prune_indices}\")\n",
    "    return new_model, out_bias, prune_indices\n",
    "\n",
    "def iAct_channel_pruning_l1_norm(model, prune_indices):\n",
    "    layer_shape = model.state_dict()['weight'].size()\n",
    "    weight_copy = model.weight.data.abs().clone()\n",
    "    \n",
    "    prune_indices = set(prune_indices)\n",
    "    overall_indices = set([i for i in range(layer_shape[0])])\n",
    "    remaining_indices = overall_indices - prune_indices\n",
    "\n",
    "    new_model = ConvTranspose2dWN(len(remaining_indices), int(layer_shape[1]), kernel_size=(4,4), stride=(2,2), padding=(1,1), bias=False)\n",
    "    \n",
    "    in_weights_float = torch.zeros((int(len(remaining_indices)), int(layer_shape[1]), int(layer_shape[2]), int(layer_shape[3])), dtype=torch.float)\n",
    "    in_weights_float = weight_copy[list(remaining_indices), :, :, :]\n",
    "    new_model.weight = torch.nn.Parameter(in_weights_float)\n",
    "    print(f\"prune input channel indice={prune_indices}, num_channels_to_prune={len(prune_indices)}, remaining_indices={remaining_indices}, prune_indices={prune_indices}\")\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepAppearanceVAE(1024, 21918, n_latent=256, n_cams=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_decoder_pruning(model, unified_pruning_ratio):\n",
    "    model.dec.texture_decoder.upsample[0].conv1.deconv, model.dec.texture_decoder.upsample[0].conv1.bias, prune_indices_1 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[0].conv1.deconv, model.dec.texture_decoder.upsample[0].conv1.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[0].conv2.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[0].conv2.deconv, prune_indices_1)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[0].conv2.deconv, model.dec.texture_decoder.upsample[0].conv2.bias, prune_indices_2 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[0].conv2.deconv, model.dec.texture_decoder.upsample[0].conv2.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[1].conv1.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[1].conv1.deconv, prune_indices_2)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[1].conv1.deconv,  model.dec.texture_decoder.upsample[1].conv1.bias, prune_indices_3 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[1].conv1.deconv, model.dec.texture_decoder.upsample[1].conv1.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[1].conv2.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[1].conv2.deconv, prune_indices_3)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[1].conv2.deconv,  model.dec.texture_decoder.upsample[1].conv2.bias, prune_indices_4 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[1].conv2.deconv, model.dec.texture_decoder.upsample[1].conv2.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[2].conv1.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[2].conv1.deconv, prune_indices_4)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[2].conv1.deconv,  model.dec.texture_decoder.upsample[2].conv1.bias, prune_indices_5 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[2].conv1.deconv, model.dec.texture_decoder.upsample[2].conv1.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[2].conv2.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[2].conv2.deconv, prune_indices_5)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[2].conv2.deconv,  model.dec.texture_decoder.upsample[2].conv2.bias, prune_indices_6 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[2].conv2.deconv, model.dec.texture_decoder.upsample[2].conv2.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[3].conv1.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[3].conv1.deconv, prune_indices_6)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[3].conv1.deconv, model.dec.texture_decoder.upsample[3].conv1.bias, prune_indices_7 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[3].conv1.deconv, model.dec.texture_decoder.upsample[3].conv1.bias, unified_pruning_ratio)\n",
    "    model.dec.texture_decoder.upsample[3].conv2.deconv = iAct_channel_pruning_l1_norm(model.dec.texture_decoder.upsample[3].conv2.deconv, prune_indices_7)\n",
    "\n",
    "    model.dec.texture_decoder.upsample[3].conv2.deconv,  model.dec.texture_decoder.upsample[3].conv2.bias, prune_indices_8 = weight_kernel_pruning_l1_norm(model.dec.texture_decoder.upsample[3].conv2.deconv, model.dec.texture_decoder.upsample[3].conv2.bias, unified_pruning_ratio)\n",
    "\n",
    "unified_pruning_ratio = 0.3\n",
    "model_decoder_pruning(unified_pruning_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "optimizer = optim.Adam(model.get_model_params(), lr, (0.9, 0.999))\n",
    "optimizer_cc = optim.Adam(model.get_cc_params(), lr, (0.9, 0.999))\n",
    "mse = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "pred_tex, pred_verts, unwarped_tex, warp_field, kl = model(in_avg_tex, in_verts, in_view, cams=in_cams)\n",
    "vert_loss = mse(pred_verts, in_verts)\n",
    "\n",
    "tex_loss = mse(pred_tex, gt_tex) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pica37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
