{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "from dataset import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from models import DeepAppearanceVAE, WarpFieldVAE\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "from utils import Renderer, gammaCorrect\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "from torch.ao.quantization import (\n",
    "  get_default_qconfig_mapping,\n",
    "  get_default_qat_qconfig_mapping,\n",
    "  QConfigMapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the actual NN model, optimizer, scheduler and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepAppearanceVAE(1024, 21918, n_latent=256, n_cams=38)\n",
    "pretrained_dict = torch.load(\"/workspace/uwing2/multiface/pretrained_model/6795937_best_base_model.pth\")\n",
    "filtered_dict = {k.replace('module.', ''): v for k, v in pretrained_dict.items() if 'module.' in k}\n",
    "model.load_state_dict(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "qconfig_mapping = get_default_qat_qconfig_mapping(\"fbgemm\")\n",
    "optimizer = optim.Adam(model.get_model_params(), 3e-4, (0.9, 0.999))\n",
    "optimizer_cc = optim.Adam(model.get_cc_params(), 3e-4, (0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking 0\n",
      "checking 1000\n",
      "checking 2000\n",
      "checking 3000\n",
      "checking 4000\n",
      "checking 5000\n",
      "checking 6000\n",
      "checking 7000\n",
      "checking 8000\n",
      "checking 9000\n",
      "checking 10000\n",
      "checking 11000\n",
      "checking 12000\n",
      "checking 13000\n"
     ]
    }
   ],
   "source": [
    "f = open(\"/workspace/uwing2/Privatar/multiface_partition_bdct4x4_nohp/camera_configs/camera-split-config_6795937.json\", \"r\")\n",
    "camera_config = json.load(f)['full']\n",
    "dataset_train = Dataset(\n",
    "    \"/workspace/uwing2/multiface/dataset/m--20180227--0000--6795937--GHS\",\n",
    "    \"/workspace/uwing2/multiface/dataset/m--20180227--0000--6795937--GHS/KRT\",\n",
    "    \"/workspace/uwing2/multiface/dataset/m--20180227--0000--6795937--GHS/frame_list.txt\",\n",
    "    1024,\n",
    "    camset=None if camera_config is None else camera_config[\"train\"],\n",
    "    exclude_prefix=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(dataset_train)\n",
    "train_loader = DataLoader(\n",
    "    dataset_train,\n",
    "    1,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_tensor = model.dec.texture_decoder.upsample[0].conv1.deconv.state_dict()['weight']#.shape#['g'].shape\n",
    "g_tensor = model.dec.texture_decoder.upsample[0].conv1.deconv.state_dict()['g']#['g'].shape\n",
    "result_tensor = weights_tensor * g_tensor[None, :, None, None]\n",
    "wnorm = torch.sqrt(torch.sum(weights_tensor**2))\n",
    "event_out_tensor = result_tensor  / wnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_out_tensor = result_tensor  / wnorm\n",
    "print(event_out_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward testing\n",
    "scale_back_tensor = event_out_tensor * wnorm / g_tensor[None, :, None, None]\n",
    "torch.equal(scale_back_tensor, weights_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Quantize the transposed convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantized_range(bitwidth):\n",
    "    quantized_max = (1 << (bitwidth - 1)) - 1\n",
    "    quantized_min = -(1 << (bitwidth - 1))\n",
    "    return quantized_min, quantized_max\n",
    "\n",
    "def get_quantization_scale_for_weight(weight, bitwidth):\n",
    "    \"\"\"\n",
    "    get quantization scale for single tensor of weight\n",
    "    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized\n",
    "    :param bitwidth: [integer] quantization bit width\n",
    "    :return:\n",
    "        [floating scalar] scale\n",
    "    \"\"\"\n",
    "    # we just assume values in weight are symmetric\n",
    "    # we also always make zero_point 0 for weight\n",
    "    fp_max = max(weight.abs().max().item(), 5e-7)\n",
    "    _, quantized_max = get_quantized_range(bitwidth)\n",
    "    return fp_max / quantized_max\n",
    "\n",
    "def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.int8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    linear quantization for single fp_tensor\n",
    "      from\n",
    "        fp_tensor = (quantized_tensor - zero_point) * scale\n",
    "      we have,\n",
    "        quantized_tensor = int(round(fp_tensor / scale)) + zero_point\n",
    "    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :param scale: [torch.(cuda.)FloatTensor] scaling factor\n",
    "    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values\n",
    "    :return:\n",
    "        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers\n",
    "    \"\"\"\n",
    "    assert(fp_tensor.dtype == torch.float)\n",
    "    assert(isinstance(scale, float) or\n",
    "           (scale.dtype == torch.float and scale.dim() == fp_tensor.dim()))\n",
    "    assert(isinstance(zero_point, int) or\n",
    "           (zero_point.dtype == dtype and zero_point.dim() == fp_tensor.dim()))\n",
    "\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # Step 1: scale the fp_tensor\n",
    "    scaled_tensor = fp_tensor / scale\n",
    "    # Step 2: round the floating value to integer value\n",
    "    rounded_tensor = torch.round(scaled_tensor)\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    rounded_tensor = rounded_tensor.to(dtype)\n",
    "\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # Step 3: shift the rounded_tensor to make zero_point 0\n",
    "    shifted_tensor = rounded_tensor + zero_point\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    # Step 4: clamp the shifted_tensor to lie in bitwidth-bit range\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    print(quantized_min, quantized_max)\n",
    "    print(torch.min(shifted_tensor), torch.max(shifted_tensor))\n",
    "    quantized_tensor = shifted_tensor.clamp_(quantized_min, quantized_max)\n",
    "    return quantized_tensor\n",
    "\n",
    "def linear_quantize_weight_per_channel(tensor, bitwidth, datatype):\n",
    "    \"\"\"\n",
    "    linear quantization for weight tensor\n",
    "        using different scales and zero_points for different output channels\n",
    "    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :return:\n",
    "        [torch.(cuda.)Tensor] quantized tensor\n",
    "        [torch.(cuda.)Tensor] scale tensor\n",
    "        [int] zero point (which is always 0)\n",
    "    \"\"\"\n",
    "    dim_output_channels = 0\n",
    "    num_output_channels = tensor.shape[dim_output_channels]\n",
    "    scale = torch.zeros(num_output_channels, device=tensor.device)\n",
    "    for oc in range(num_output_channels):\n",
    "        _subtensor = tensor.select(dim_output_channels, oc)\n",
    "        _scale = get_quantization_scale_for_weight(_subtensor, bitwidth)\n",
    "        scale[oc] = _scale\n",
    "    scale_shape = [1] * tensor.dim()\n",
    "    scale_shape[dim_output_channels] = -1\n",
    "    scale = scale.view(scale_shape)\n",
    "    quantized_tensor = linear_quantize(tensor, bitwidth, scale, zero_point=0, dtype=datatype)\n",
    "    return quantized_tensor, scale, 0\n",
    "\n",
    "def linear_quantize_and_replace_weight(conv_transpose_layer, bitwidth=16, datatype=torch.int16):\n",
    "    new_state_dict = conv_transpose_layer.state_dict()\n",
    "    weights_tensor = torch.clone(new_state_dict['weight'])\n",
    "    wnorm = torch.sqrt(torch.sum(weights_tensor**2))\n",
    "    g_tensor = new_state_dict['g']#['g'].shape\n",
    "    result_tensor = weights_tensor * g_tensor[None, :, None, None]\n",
    "    event_out_tensor = result_tensor  / wnorm\n",
    "    post_quantized_tensor, scale, zp = linear_quantize_weight_per_channel(event_out_tensor, bitwidth, datatype)\n",
    "    post_quantized_tensor_fp = post_quantized_tensor.float() #torch.convert(post_quantized_tensor, torch.float)\n",
    "    post_quantized_tensor_fp = (post_quantized_tensor_fp - zp) * scale\n",
    "    post_quantized_weights = post_quantized_tensor_fp * wnorm / g_tensor[None, :, None, None]\n",
    "    new_state_dict['weight'] = post_quantized_weights\n",
    "    conv_transpose_layer.load_state_dict(new_state_dict)\n",
    "    return conv_transpose_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8192 8191\n",
      "tensor(-8191, dtype=torch.int32) tensor(8191, dtype=torch.int32)\n",
      "-8192 8191\n",
      "tensor(-8191, dtype=torch.int32) tensor(8191, dtype=torch.int32)\n",
      "-8192 8191\n",
      "tensor(-8191, dtype=torch.int32) tensor(8191, dtype=torch.int32)\n",
      "-8192 8191\n",
      "tensor(-8191, dtype=torch.int32) tensor(8191, dtype=torch.int32)\n",
      "-8192 8191\n",
      "tensor(-8191, dtype=torch.int32) tensor(8191, dtype=torch.int32)\n",
      "-8192 8191\n",
      "tensor(-8191, dtype=torch.int32) tensor(8191, dtype=torch.int32)\n",
      "-8192 8191\n",
      "tensor(-8191, dtype=torch.int32) tensor(8191, dtype=torch.int32)\n",
      "-8192 8191\n",
      "tensor(-8191, dtype=torch.int32) tensor(8191, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "bitwidth = 14\n",
    "datatype = torch.int16\n",
    "for i in range(len(model.dec.texture_decoder.upsample)):\n",
    "    linear_quantize_and_replace_weight(model.dec.texture_decoder.upsample[i].conv1.deconv, bitwidth, datatype)\n",
    "    linear_quantize_and_replace_weight(model.dec.texture_decoder.upsample[i].conv2.deconv, bitwidth, datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "for i, data in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_cc.zero_grad()\n",
    "    \n",
    "    M = data[\"M\"]#.cuda()\n",
    "    gt_tex = data[\"tex\"]#.cuda()\n",
    "    vert_ids = data[\"vert_ids\"]#.cuda()\n",
    "    uvs = data[\"uvs\"]#.cuda()\n",
    "    uv_ids = data[\"uv_ids\"]#.cuda()\n",
    "    avg_tex = data[\"avg_tex\"]#.cuda()\n",
    "    view = data[\"view\"]#.cuda()\n",
    "    transf = data[\"transf\"]#.cuda()\n",
    "    verts = data[\"aligned_verts\"]#.cuda()\n",
    "    photo = data[\"photo\"]#.cuda()\n",
    "    mask = data[\"mask\"]#.cuda()\n",
    "    cams = data[\"cam\"]#.cuda()\n",
    "    batch, channel, height, width = avg_tex.shape\n",
    "    output = {}\n",
    "    height_render, width_render = [2048, 1334]\n",
    "    width_render = width_render - (width_render % 8)\n",
    "    photo_short = torch.Tensor(photo)[:, :, :width_render, :]\n",
    "\n",
    "    pred_tex, pred_verts, kl = model(avg_tex, verts, view, cams=cams)\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "    optimizer.step()\n",
    "    optimizer_cc.step()\n",
    "    loss = mse(pred_tex, avg_tex)\n",
    "    loss.backward()\n",
    "    if (i > 5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Quantization Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dec.texture_decoder.module.upsample[0].conv1.deconv.state_dict()['g']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Transposed Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 12, 12])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With square kernels and equal stride\n",
    "m = nn.ConvTranspose2d(16, 33, 3, stride=2)\n",
    "# non-square kernels and unequal stride and with padding\n",
    "m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "input = torch.randn(20, 16, 50, 100)\n",
    "output = m(input)\n",
    "# exact output size can be also specified as an argument\n",
    "input = torch.randn(1, 16, 12, 12)\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "h = downsample(input)\n",
    "h.size()\n",
    "output = upsample(h, output_size=input.size())\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[[[-3.8928e-02, -4.4176e-02,  3.6679e-02, -1.2662e-02, -3.5243e-02],\n",
       "                        [-3.6266e-02,  1.9824e-02,  1.2112e-02,  6.6716e-03, -6.2657e-03],\n",
       "                        [ 3.5419e-02, -1.4695e-03, -3.2812e-02,  1.1015e-02,  7.7270e-03]],\n",
       "              \n",
       "                       [[-4.2148e-02, -8.9569e-03,  4.2684e-03,  2.7824e-02,  1.9044e-02],\n",
       "                        [ 3.6119e-02, -5.7129e-03, -2.0307e-02, -4.3224e-02, -4.0367e-02],\n",
       "                        [-1.2355e-02,  7.8870e-03, -2.0248e-03,  2.8258e-02, -4.1270e-02]],\n",
       "              \n",
       "                       [[ 2.2233e-02, -2.2651e-02, -2.5109e-02,  2.0934e-02, -2.0716e-02],\n",
       "                        [-1.5664e-03,  6.5778e-03, -4.1291e-02, -4.0083e-03, -1.5251e-02],\n",
       "                        [-2.1261e-02,  4.1453e-02,  1.3748e-02, -3.5007e-02,  1.7711e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.7797e-02,  1.8618e-02, -1.0890e-02,  2.7789e-02,  2.1815e-02],\n",
       "                        [-1.7093e-02, -1.2454e-02, -4.3873e-02, -6.8374e-03, -9.3563e-03],\n",
       "                        [ 1.4356e-02,  9.8113e-03,  4.3733e-03,  2.3813e-02,  4.2610e-02]],\n",
       "              \n",
       "                       [[ 4.1614e-02, -4.2363e-02,  3.0857e-02, -3.2207e-03,  2.6263e-02],\n",
       "                        [ 1.4047e-02, -2.4822e-03,  2.9666e-02, -3.1226e-02,  1.3295e-02],\n",
       "                        [-3.5570e-02, -3.1815e-02, -1.8276e-02,  3.1386e-02, -3.9784e-02]],\n",
       "              \n",
       "                       [[ 1.8474e-02,  6.6709e-03,  4.1876e-02,  1.8026e-02, -2.5156e-02],\n",
       "                        [ 3.6687e-02,  1.8535e-02,  3.7008e-02, -6.9426e-04,  3.8439e-03],\n",
       "                        [-4.4848e-02, -2.7842e-02, -3.0206e-02,  7.0985e-03, -9.5129e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.4838e-02,  2.3228e-02, -1.6477e-02, -3.8637e-02,  2.9581e-02],\n",
       "                        [-1.7255e-02,  2.0586e-03,  2.3861e-02,  1.4089e-02, -1.9741e-03],\n",
       "                        [-4.4847e-02,  3.1361e-02, -4.1126e-02,  3.3241e-02,  2.0463e-02]],\n",
       "              \n",
       "                       [[-2.0746e-02, -2.9506e-02,  3.9046e-02,  1.5487e-02,  8.6183e-03],\n",
       "                        [-3.6332e-02,  4.2800e-02, -4.9961e-03, -8.1363e-03,  1.0936e-02],\n",
       "                        [-3.9217e-02,  4.0109e-02, -2.6305e-02, -2.9673e-02,  2.7276e-02]],\n",
       "              \n",
       "                       [[-2.2843e-02, -1.0085e-02, -4.0866e-02,  4.3718e-02, -4.2071e-02],\n",
       "                        [-3.0994e-02, -2.3302e-02, -8.3372e-03, -3.8485e-02,  3.8183e-03],\n",
       "                        [ 1.6825e-03,  4.7777e-03, -1.1104e-02,  3.9216e-02,  7.9907e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.7794e-03, -3.8209e-02,  6.2437e-03,  1.3515e-02, -3.4857e-02],\n",
       "                        [-3.2806e-02, -1.5925e-02,  2.8296e-02, -4.3600e-02,  3.3869e-02],\n",
       "                        [-3.0803e-02, -2.5840e-02, -2.9726e-02, -3.8032e-02, -2.6758e-02]],\n",
       "              \n",
       "                       [[-1.7379e-02, -3.9776e-02,  1.4532e-02,  4.1838e-02,  2.6736e-02],\n",
       "                        [ 4.2699e-02, -1.0296e-02, -1.4147e-02, -2.3982e-02,  2.8059e-02],\n",
       "                        [ 1.1211e-02,  1.5160e-02,  3.4829e-02, -2.9665e-02,  2.9547e-02]],\n",
       "              \n",
       "                       [[-3.3428e-02, -3.8521e-03,  1.7009e-02, -2.9916e-02, -4.4583e-02],\n",
       "                        [ 1.4429e-02, -1.9991e-02, -2.5103e-02, -3.3804e-02,  4.1561e-02],\n",
       "                        [ 7.7592e-03,  3.5254e-03,  1.8662e-02, -3.0668e-02, -2.6882e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5251e-02,  2.8689e-02,  3.0824e-02,  3.6619e-02, -4.1218e-02],\n",
       "                        [ 1.2851e-02, -1.9440e-02, -3.7439e-02, -1.5381e-02,  3.0071e-03],\n",
       "                        [ 3.6231e-03,  8.5555e-03, -3.7668e-02, -2.0083e-02,  1.6923e-02]],\n",
       "              \n",
       "                       [[-3.3163e-02, -2.2731e-02,  2.3791e-02,  2.5888e-02,  1.6651e-02],\n",
       "                        [ 1.0051e-02, -3.6958e-02, -3.4910e-02, -3.0488e-02,  2.4132e-02],\n",
       "                        [-8.5271e-03,  2.0488e-02, -4.3971e-02, -4.1636e-02, -4.2743e-02]],\n",
       "              \n",
       "                       [[-3.0606e-02,  4.4148e-02, -5.9599e-03,  3.5581e-02, -8.4840e-03],\n",
       "                        [ 4.1215e-02,  4.0211e-02,  4.2653e-02,  3.3854e-02,  2.9565e-03],\n",
       "                        [-4.8281e-03, -7.0849e-03,  2.7167e-02, -4.1617e-02, -4.4584e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.0318e-02,  9.4772e-03,  5.4428e-03, -3.1004e-02,  2.6281e-02],\n",
       "                        [-4.3837e-02, -1.0220e-02, -4.0181e-02,  2.7516e-02,  2.7914e-02],\n",
       "                        [ 1.0414e-02,  4.1821e-02,  4.1953e-02, -1.3745e-02,  1.8276e-02]],\n",
       "              \n",
       "                       [[-3.7423e-02, -3.0567e-03, -2.8313e-02,  2.6960e-02, -2.9075e-02],\n",
       "                        [ 9.2925e-03,  9.4043e-03, -4.0025e-02, -3.1574e-02,  1.9870e-02],\n",
       "                        [-3.1917e-02, -1.5854e-02, -1.2278e-02, -4.7373e-03,  1.9916e-02]],\n",
       "              \n",
       "                       [[-4.0646e-02, -1.4747e-02,  4.4419e-02, -3.9006e-02,  1.0900e-02],\n",
       "                        [ 3.9545e-02,  4.2251e-02,  4.3505e-02,  1.0571e-02,  5.9797e-03],\n",
       "                        [ 9.7024e-03,  9.4182e-04, -3.2624e-02,  6.1539e-03, -1.4978e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.5539e-03,  1.8323e-02, -4.3987e-02,  3.2286e-02, -3.5033e-02],\n",
       "                        [ 3.4915e-02,  3.6487e-02,  3.0105e-02,  8.5605e-03, -1.7087e-05],\n",
       "                        [ 8.0227e-04,  1.1992e-02,  3.8317e-02, -9.8940e-03,  9.7498e-03]],\n",
       "              \n",
       "                       [[ 1.3166e-02,  1.0304e-02,  4.0086e-03, -6.1874e-03, -2.0761e-02],\n",
       "                        [ 9.6854e-03,  2.7470e-02, -2.8397e-02,  4.3818e-02, -1.4367e-03],\n",
       "                        [-1.3511e-02, -3.4472e-02, -2.4227e-02,  1.1012e-02, -9.5244e-03]],\n",
       "              \n",
       "                       [[-1.0196e-02,  1.4988e-02, -1.1917e-02,  8.8166e-03, -3.5697e-02],\n",
       "                        [-4.0993e-03,  3.7207e-02, -8.2983e-03,  2.4670e-02,  8.6778e-03],\n",
       "                        [ 1.4615e-02,  1.1471e-03,  8.3427e-03, -1.4976e-02, -8.1362e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.2708e-02, -3.0408e-02, -3.4957e-02, -1.4315e-02, -2.8867e-02],\n",
       "                        [ 7.4882e-03, -3.9514e-02,  3.8152e-02, -2.2863e-02,  1.5483e-03],\n",
       "                        [ 3.1812e-03, -4.4496e-02, -2.7517e-02, -1.5791e-02,  4.4509e-02]],\n",
       "              \n",
       "                       [[-2.8013e-02,  3.9677e-02,  1.4685e-03,  1.3524e-02,  2.0378e-03],\n",
       "                        [ 4.4356e-02,  1.8118e-02,  2.8993e-02,  1.7345e-02,  3.2316e-03],\n",
       "                        [ 3.2364e-02,  9.3709e-03, -4.7753e-04, -7.2425e-03,  8.8333e-03]],\n",
       "              \n",
       "                       [[-7.3950e-03, -1.5606e-02,  1.6496e-02, -3.1278e-03, -3.6486e-02],\n",
       "                        [-1.1374e-02, -2.0323e-03, -2.8066e-02,  1.4613e-02, -3.3623e-03],\n",
       "                        [-2.8833e-02,  8.0709e-03,  8.6866e-03, -7.5716e-03, -3.2955e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.4946e-02, -3.3905e-02,  1.3330e-02,  2.0002e-02, -2.4317e-02],\n",
       "                        [-2.9858e-02, -4.0562e-02, -4.6415e-03, -2.8125e-02,  1.2533e-03],\n",
       "                        [ 6.1628e-03, -1.2961e-02, -4.0546e-02,  3.1526e-02, -2.6734e-02]],\n",
       "              \n",
       "                       [[-1.8520e-03, -1.5576e-02, -1.8630e-02,  3.8850e-02, -1.1911e-02],\n",
       "                        [-3.2024e-02, -7.2699e-03, -4.7999e-03, -2.7602e-02,  1.2328e-04],\n",
       "                        [-2.5577e-02,  1.9931e-02, -1.6855e-02,  5.4271e-03, -2.5035e-02]],\n",
       "              \n",
       "                       [[-1.3064e-02,  3.1648e-02,  3.8646e-02,  1.1207e-02, -7.2429e-03],\n",
       "                        [-1.5326e-02, -8.3412e-03, -1.3178e-02, -3.1643e-02,  3.5654e-02],\n",
       "                        [-2.5232e-02, -1.3204e-02, -1.0351e-02,  4.2228e-02, -3.6021e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.6839e-02,  4.4526e-02,  7.1256e-04,  3.3518e-02, -3.4145e-02],\n",
       "                        [-4.0394e-02, -3.6533e-02, -3.8080e-03, -3.7064e-02, -9.6074e-03],\n",
       "                        [-1.4042e-02,  3.5650e-02, -3.7530e-02, -8.0936e-03, -4.2330e-02]],\n",
       "              \n",
       "                       [[-3.7608e-03,  2.9690e-02,  6.3865e-03,  2.1695e-02,  4.0063e-02],\n",
       "                        [-2.9082e-02, -7.0473e-03, -8.7550e-03,  1.5382e-02, -7.3954e-03],\n",
       "                        [ 4.0799e-02, -3.9677e-02,  1.5373e-02,  2.8292e-02,  6.3996e-03]],\n",
       "              \n",
       "                       [[-3.1251e-02, -3.7038e-02,  2.2751e-02, -1.1311e-02,  3.0087e-02],\n",
       "                        [-2.7537e-02,  8.5032e-03,  2.8936e-02, -2.6694e-02, -1.0398e-02],\n",
       "                        [ 2.4177e-03, -7.5081e-03, -6.7599e-03, -2.4434e-02, -1.4528e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.3985e-02, -2.0075e-02, -2.9512e-02,  3.5384e-02,  4.1462e-03],\n",
       "                        [ 3.8953e-03,  2.7495e-02, -4.3874e-02, -3.8282e-02, -2.5676e-03],\n",
       "                        [ 4.4498e-02, -4.2778e-02,  1.0602e-02, -2.9491e-02,  5.3487e-03]],\n",
       "              \n",
       "                       [[-2.5132e-02, -3.1166e-02,  1.1144e-02, -4.2965e-02, -5.3442e-03],\n",
       "                        [ 3.9637e-02,  5.1909e-03, -3.0781e-02, -1.3457e-02, -4.0542e-02],\n",
       "                        [-2.6202e-02, -5.0906e-03, -1.1453e-02,  3.3668e-02, -1.2034e-02]],\n",
       "              \n",
       "                       [[-3.6590e-02, -4.1004e-02, -2.6352e-02, -3.8755e-02,  1.1551e-02],\n",
       "                        [ 9.5524e-03, -4.4570e-02,  2.5545e-02, -3.9270e-02, -2.9774e-02],\n",
       "                        [ 1.1341e-02,  1.8675e-03, -1.0925e-03, -3.5922e-03,  1.3140e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.3422e-03,  3.1720e-02,  1.8027e-02,  2.9957e-02, -4.1860e-02],\n",
       "                        [-2.4906e-02,  2.8527e-02,  2.4257e-02, -1.1847e-02, -8.7777e-03],\n",
       "                        [ 1.9738e-02,  3.7296e-03,  1.8627e-02, -4.3022e-02, -2.0012e-02]],\n",
       "              \n",
       "                       [[ 2.8702e-02, -2.8691e-02, -2.3425e-02,  4.0707e-02,  1.5946e-02],\n",
       "                        [ 1.5262e-02,  1.2989e-02, -1.2844e-02, -5.4408e-04, -5.9405e-03],\n",
       "                        [-3.4498e-02,  1.0754e-02, -2.2610e-02, -2.3992e-02, -2.4206e-02]],\n",
       "              \n",
       "                       [[ 3.0127e-02, -3.3043e-02,  2.4330e-02,  1.0732e-02,  4.2756e-02],\n",
       "                        [ 4.0432e-02,  3.2666e-02,  1.2813e-02,  5.4813e-03, -3.2887e-02],\n",
       "                        [ 2.1150e-03, -1.0792e-02, -1.9438e-02,  2.8271e-02, -1.9237e-02]]]])),\n",
       "             ('bias',\n",
       "              tensor([-0.0072, -0.0040, -0.0151,  0.0142, -0.0253, -0.0138, -0.0211, -0.0273,\n",
       "                       0.0018,  0.0409, -0.0020, -0.0359,  0.0323, -0.0143,  0.0287,  0.0299,\n",
       "                      -0.0262,  0.0174, -0.0295, -0.0248,  0.0277,  0.0182,  0.0306, -0.0157,\n",
       "                      -0.0040,  0.0161,  0.0131, -0.0046, -0.0175, -0.0181, -0.0188,  0.0416,\n",
       "                      -0.0016]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
