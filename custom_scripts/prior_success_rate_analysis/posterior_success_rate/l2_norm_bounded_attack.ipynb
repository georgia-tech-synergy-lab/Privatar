{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianming/conda/envs/pica37/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchjpeg import dct\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:37: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:27: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:37: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:27: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:37: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/tmp/ipykernel_671597/536562697.py:27: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(x.shape[1] == 3, \"Wrong input, Channel should equals to 3\")\n",
      "/tmp/ipykernel_671597/536562697.py:37: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(x.shape[1] == 3, \"Wrong input, Channel should equals to 3\")\n"
     ]
    }
   ],
   "source": [
    "block_size = 4\n",
    "total_frequency_component = block_size * block_size\n",
    "\n",
    "overall_img_path_list = []\n",
    "path_prefix = \"/home/jianming/work/multiface/dataset/m--20180227--0000--6795937--GHS/unwrapped_uv_1024/\"\n",
    "all_dir = os.listdir(path_prefix)\n",
    "for sgl_dir in all_dir:\n",
    "    path_average = os.path.join(path_prefix + sgl_dir, \"average\")\n",
    "    overall_img_path_list.append(os.path.join(path_average, os.listdir(path_average)[0]))\n",
    "\n",
    "overall_img_path_list2 = []\n",
    "path_prefix2 = \"/scratch1/jianming/multiface/dataset/m--20180226--0000--6674443--GHS/unwrapped_uv_1024/\"\n",
    "all_dir = os.listdir(path_prefix2)\n",
    "for sgl_dir in all_dir:\n",
    "    path_average2 = os.path.join(path_prefix2 + sgl_dir, \"average\")\n",
    "    overall_img_path_list2.append(os.path.join(path_average2, os.listdir(path_average2)[0]))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "downsample_components_list = []\n",
    "for img_path in overall_img_path_list:\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    x = transform(image).unsqueeze(0)\n",
    "    x = (x + 1) / 2 * 255\n",
    "    assert(x.shape[1] == 3, \"Wrong input, Channel should equals to 3\")\n",
    "    original_in_img = dct.to_ycbcr(x)  # comvert RGB to YCBCR\n",
    "    original_in_img -= 128\n",
    "    downsample_components_list.append(original_in_img)\n",
    "\n",
    "downsample_components_list2 = []\n",
    "for img_path in overall_img_path_list2:\n",
    "    image2 = Image.open(img_path).convert('RGB')\n",
    "    x = transform(image2).unsqueeze(0)\n",
    "    x = (x + 1) / 2 * 255\n",
    "    assert(x.shape[1] == 3, \"Wrong input, Channel should equals to 3\")\n",
    "    original_in_img2 = dct.to_ycbcr(x)  # comvert RGB to YCBCR\n",
    "    original_in_img2 -= 128\n",
    "    downsample_components_list2.append(original_in_img2)\n",
    "\n",
    "downsample_components_overall = downsample_components_list + downsample_components_list2\n",
    "# L2 norm among highest frequency components after BDCT decomposition\n",
    "num_images = len(downsample_components_overall)\n",
    "\n",
    "l2_norm_expression_list_overall = np.zeros((len(downsample_components_overall), len(downsample_components_overall)))\n",
    "for i in range(num_images):\n",
    "    for j in range(num_images):\n",
    "        l2_norm_expression_list_overall[i][j] = np.linalg.norm(downsample_components_overall[i] - downsample_components_overall[j])\n",
    "np.save(f\"l2_norm_in_img_component.npy\", l2_norm_expression_list_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131, 131)\n",
      "13191.1337890625\n"
     ]
    }
   ],
   "source": [
    "print(l2_norm_expression_list_overall.shape)\n",
    "print(np.max(l2_norm_expression_list_overall))\n",
    "max_l2_norm = np.max(l2_norm_expression_list_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.007633587786259542, 0.007633587786259542, 0.007633587786259542, 0.007633587786259542, 0.007633587786259542, 0.007633587786259542, 0.015267175572519083, 0.015267175572519083, 0.015267175572519083, 0.022900763358778626, 0.030534351145038167, 0.04580152671755725, 0.08396946564885496, 0.13740458015267176, 0.19083969465648856, 0.24427480916030533, 0.2748091603053435, 0.32061068702290074, 0.33587786259541985, 0.35877862595419846, 0.37404580152671757, 0.3893129770992366, 0.4122137404580153, 0.42748091603053434, 0.44274809160305345, 0.45038167938931295, 0.46564885496183206, 0.4732824427480916, 0.48091603053435117, 0.4961832061068702, 0.5038167938931297, 0.5038167938931297, 0.5038167938931297, 0.5038167938931297, 0.5038167938931297, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.5114503816793893, 0.6412213740458015, 0.8778625954198473, 0.9847328244274809, 0.9923664122137404, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# divide overall l2 norm into 10 equally splitted pieces and then fix the PSR at 0.25 -> obtain the final reconstructured L2 norm.\n",
    "def calculate_prior_sr(target_l2norm, l2_norm_expression_list_overall):\n",
    "    overall_category = l2_norm_expression_list_overall.shape[0]\n",
    "\n",
    "    per_list_statistics = []\n",
    "    for i in range(l2_norm_expression_list_overall.shape[0]):\n",
    "        per_list_statistics.append(np.where(l2_norm_expression_list_overall[i,:] < target_l2norm))\n",
    "\n",
    "    selective_list= []\n",
    "    for per_row in per_list_statistics:\n",
    "        selective_list.append(len(per_row[0]))\n",
    "    \n",
    "    pri_sr = np.max(selective_list)/overall_category\n",
    "\n",
    "    return pri_sr\n",
    "\n",
    "pri_sr_list = []\n",
    "for i in range(100):\n",
    "    target_l2norm = max_l2_norm * i / 100\n",
    "    \n",
    "    # calculate prior successful rate \n",
    "    pri_sr = calculate_prior_sr(target_l2norm, l2_norm_expression_list_overall)\n",
    "    pri_sr_list.append(pri_sr)\n",
    "\n",
    "print(pri_sr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 131.911337890625, 263.82267578125, 395.734013671875, 527.6453515625, 659.556689453125, 791.46802734375, 923.379365234375, 1055.290703125, 1187.202041015625, 1319.11337890625, 1451.024716796875, 1582.9360546875, 1714.847392578125, 1846.75873046875, 1978.670068359375, 2110.58140625]\n"
     ]
    }
   ],
   "source": [
    "selected_pri_sr_list = np.array(pri_sr_list)[np.where(np.array(pri_sr_list) < 0.2)[0]]\n",
    "selected_l2_norm_list = []\n",
    "for i in range(100):\n",
    "    target_l2norm = max_l2_norm * i / 100\n",
    "    if len(selected_l2_norm_list) <= len(selected_pri_sr_list):\n",
    "        selected_l2_norm_list.append(target_l2norm)\n",
    "print(selected_l2_norm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 0.13862943611198905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianming/conda/envs/pica37/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/jianming/conda/envs/pica37/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import rel_entr\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Calculate the Kullback-Leibler (KL) divergence between two probability distributions.\n",
    "    \n",
    "    Args:\n",
    "    p (array-like): The true probability distribution (must be a valid probability distribution).\n",
    "    q (array-like): The estimated probability distribution (must be a valid probability distribution).\n",
    "    \n",
    "    Returns:\n",
    "    float: The KL divergence between distributions p and q.\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=np.float)\n",
    "    q = np.asarray(q, dtype=np.float)\n",
    "    \n",
    "    # Ensure that neither p nor q contains zero values\n",
    "    p = np.clip(p, 1e-10, 1)\n",
    "    q = np.clip(q, 1e-10, 1)\n",
    "    \n",
    "    # Calculate KL divergence\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "# Example usage\n",
    "p = [0.2]\n",
    "q = [0.1]\n",
    "\n",
    "kl_div = kl_divergence(p, q)\n",
    "print(f\"KL Divergence: {kl_div}\")\n",
    "\n",
    "def calculate_edge_kl_divergence(prior, mi):\n",
    "    posterior = prior\n",
    "    posterior_pre = prior\n",
    "    for test_val in range(100):\n",
    "        posterior = test_val*(1-prior)/100 + prior\n",
    "        if kl_divergence(posterior_pre, prior) < mi and kl_divergence(posterior, prior) >= mi:\n",
    "            return posterior\n",
    "    return 1\n",
    "\n",
    "def calculate_mi_bound(prior, posterior_target):\n",
    "    posterior = posterior_target\n",
    "    for test_val in range(10000):\n",
    "        mi = test_val / 1000\n",
    "        mi_next = (test_val + 1) / 1000\n",
    "\n",
    "        if kl_divergence(posterior, prior) < mi_next and kl_divergence(posterior, prior) >= mi:\n",
    "            return mi\n",
    "    return -1\n",
    "\n",
    "def obtain_mi_list_under_various_prior_rate(selected_pri_sr_list, posterior_target):\n",
    "    selected_mi_list = []\n",
    "    for pri_sr in selected_pri_sr_list:\n",
    "        selected_mi_list.append(calculate_mi_bound(pri_sr, posterior_target=posterior_target))\n",
    "    return selected_mi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianming/conda/envs/pica37/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/jianming/conda/envs/pica37/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# obtain the mutual information list of mutual information for getting a specific PSR\n",
    "selected_mi_list = obtain_mi_list_under_various_prior_rate(selected_pri_sr_list, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_covariance_matrix(number_files, batch_size, captured_data_list):\n",
    "    captured_texture_avg_data = np.zeros(((number_files-1)*batch_size, 256))\n",
    "\n",
    "    for i in range(number_files-1):\n",
    "        texture_avg_file_list = f\"{captured_data_list}/z_{i+1}.pth\"\n",
    "        captured_texture_avg = torch.load(texture_avg_file_list).to(\"cpu\")\n",
    "        captured_texture_avg_data[i*batch_size:(i+1)*batch_size] = captured_texture_avg.detach().numpy()\n",
    "    covariance_matrix_texture_avg = np.cov(captured_texture_avg_data, rowvar=False)\n",
    "    return covariance_matrix_texture_avg, captured_texture_avg_data\n",
    "\n",
    "def calculate_covariance_matrix_outsource_latent_code(number_files, batch_size, captured_data_list):\n",
    "    captured_texture_avg_data = np.zeros(((number_files-1)*batch_size, 256))\n",
    "\n",
    "    for i in range(number_files-1):\n",
    "        texture_avg_file_list = f\"{captured_data_list}/texture_avg_{i+1}.pth\"\n",
    "        captured_texture_avg = torch.load(texture_avg_file_list).to(\"cpu\")\n",
    "        captured_texture_avg_data[i*batch_size:(i+1)*batch_size] = captured_texture_avg.detach().numpy()\n",
    "    covariance_matrix_texture_avg = np.cov(captured_texture_avg_data, rowvar=False)\n",
    "    return covariance_matrix_texture_avg\n",
    "\n",
    "def plot_eigenvalue_covariance_matrix(covariance_matrix_texture_avg, img_save_path):\n",
    "    U, s, V = np.linalg.svd(covariance_matrix_texture_avg)\n",
    "\n",
    "    X = [i for i in range(covariance_matrix_texture_avg.shape[0])]\n",
    "\n",
    "    # Data for plotting\n",
    "    SMALL_SIZE = 20\n",
    "    MEDIUM_SIZE = 22\n",
    "    BIGGER_SIZE = 24\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.plot(X, s)\n",
    "    plt.title('SVD decomposition of Cov(Z): (Z vector)', fontsize=SMALL_SIZE)\n",
    "\n",
    "    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=SMALL_SIZE)     # fontsize of the x and y labels\n",
    "    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "    plt.savefig(f'{img_save_path}.pdf', bbox_inches=\"tight\", transparent=True) \n",
    "    \n",
    "# calculate noise of each dimensions\n",
    "def generate_noise_covariance(mutual_info_bound, s_cov):\n",
    "    overall_result_cov = 0\n",
    "    for ele in s_cov:\n",
    "        overall_result_cov = overall_result_cov + np.sqrt(ele)\n",
    "    print()\n",
    "    print(f\"sum of sqrt={overall_result_cov}\")\n",
    "    print(f\"mutual information = 1 -- noise std variance={overall_result_cov/np.sqrt(2)}\")\n",
    "    noise_variance = np.zeros(256)\n",
    "    for i in range(256):\n",
    "        print(f\"s_cov[i]={s_cov[i]}, overall_result_cov={overall_result_cov}, div={(2 * mutual_info_bound)}\")\n",
    "        noise_variance[i] =  (np.sqrt(s_cov[i]) * overall_result_cov) / (2 * mutual_info_bound)\n",
    "    return noise_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the amount of injected noise under different number of outsourced components\n",
    "## Step 1: Obtain the outsourced latent code and then calculate overall amount of noises in need of injection into the framework.\n",
    "## Calculate the covariance of various different horizontal partitioning setup\n",
    "captured_data_list = \"asd\"\n",
    "number_files = 16\n",
    "batch_size = 1\n",
    "calculate_covariance_matrix()\n",
    "\n",
    "## Step 2: calculate the amount of noises that should be injected into the outsourced data\n",
    "for mi in selected_mi_list:\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pica37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
