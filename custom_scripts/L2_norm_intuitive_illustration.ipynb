{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianming/conda/envs/pica37/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchjpeg import dct\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:37: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:15: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:37: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:15: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:37: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/tmp/ipykernel_2233052/470356363.py:15: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(x.shape[1] == 3, \"Wrong input, Channel should equals to 3\")\n",
      "/tmp/ipykernel_2233052/470356363.py:37: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(downsample_img.shape == freq_block[:,:,0,:,:].shape, \"downsample input shape does not match the shape of post-BDCT component\")\n"
     ]
    }
   ],
   "source": [
    "block_size = 4\n",
    "total_frequency_component = block_size * block_size\n",
    "\n",
    "overall_img_path_list = []\n",
    "path_prefix = \"/home/jianming/work/multiface/dataset/m--20180227--0000--6795937--GHS/unwrapped_uv_1024/\"\n",
    "all_dir = os.listdir(path_prefix)\n",
    "# print(all_dir)\n",
    "for sgl_dir in all_dir:\n",
    "    path_average = os.path.join(path_prefix + sgl_dir, \"average\")\n",
    "        # print(os.path.join(path_average, image))\n",
    "    overall_img_path_list.append(os.path.join(path_average, os.listdir(path_average)[0]))\n",
    "\n",
    "def img_reorder(x, bs, ch, h, w):\n",
    "    x = (x + 1) / 2 * 255\n",
    "    assert(x.shape[1] == 3, \"Wrong input, Channel should equals to 3\")\n",
    "    x = dct.to_ycbcr(x)  # comvert RGB to YCBCR\n",
    "    x -= 128\n",
    "    x = x.view(bs * ch, 1, h, w)\n",
    "    x = F.unfold(x, kernel_size=(block_size, block_size), dilation=1, padding=0, stride=(block_size, block_size))\n",
    "    x = x.transpose(1, 2)\n",
    "    x = x.view(bs, ch, -1, block_size, block_size)\n",
    "    return x\n",
    "\n",
    "## Image reordering and testing\n",
    "def img_inverse_reroder(coverted_img, bs, ch, h, w):\n",
    "    x = coverted_img.view(bs* ch, -1, total_frequency_component)\n",
    "    x = x.transpose(1, 2)\n",
    "    x = F.fold(x, output_size=(h, w), kernel_size=(block_size, block_size), stride=(block_size, block_size))\n",
    "    x += 128\n",
    "    x = x.view(bs, ch, h, w)\n",
    "    x = dct.to_rgb(x)#.squeeze(0)\n",
    "    x = (x / 255.0) * 2 - 1\n",
    "    return x\n",
    "\n",
    "def calculate_block_mse(downsample_in, freq_block, num_freq_component=block_size):\n",
    "    downsample_img = transforms.Resize(size=int(downsample_in.shape[-1]/num_freq_component))(downsample_in)\n",
    "    assert(downsample_img.shape == freq_block[:,:,0,:,:].shape, \"downsample input shape does not match the shape of post-BDCT component\")\n",
    "    loss_vector = torch.zeros(freq_block.shape[2])\n",
    "    for i in range(freq_block.shape[2]):\n",
    "        # calculate the MSE between each frequency components and given input downsampled images\n",
    "        loss_vector[i] = F.mse_loss(downsample_img, freq_block[:,:,i,:,:])\n",
    "    return loss_vector\n",
    "\n",
    "def bdct_4x4(img_path):\n",
    "    # The original input image comes with it and I disable it to reduce the computation overhead.\n",
    "    # x = F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=True)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    x = transform(image).unsqueeze(0)\n",
    "\n",
    "    back_input = x\n",
    "    bs, ch, h, w = x.shape\n",
    "    block_num = h // block_size\n",
    "    x = img_reorder(x, bs, ch, h, w)\n",
    "    dct_block = dct.block_dct(x) # BDCT\n",
    "    dct_block_reorder = dct_block.view(bs, ch, block_num, block_num, total_frequency_component).permute(0, 1, 4, 2, 3) # into (bs, ch, 64, block_num, block_num)\n",
    "\n",
    "    return  dct_block_reorder\n",
    "\n",
    "def private_freq_component_thres_based_selection(img_path, mse_threshold):\n",
    "    # The original input image comes with it and I disable it to reduce the computation overhead.\n",
    "    # x = F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=True)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    x = transform(image).unsqueeze(0)\n",
    "\n",
    "    back_input = x\n",
    "    bs, ch, h, w = x.shape\n",
    "    block_num = h // block_size\n",
    "    x = img_reorder(x, bs, ch, h, w)\n",
    "    dct_block = dct.block_dct(x) # BDCT\n",
    "    dct_block_reorder = dct_block.view(bs, ch, block_num, block_num, total_frequency_component).permute(0, 1, 4, 2, 3) # into (bs, ch, 64, block_num, block_num)\n",
    "    loss_vector = calculate_block_mse(back_input, dct_block_reorder)\n",
    "    # Split all component based on the frequency\n",
    "    private_idx = torch.where(loss_vector > mse_threshold)[0]\n",
    "    public_idx = []\n",
    "    all_possible_idx = [i for i in range(total_frequency_component)]\n",
    "    for element in all_possible_idx:\n",
    "        if element not in private_idx:\n",
    "            public_idx.append(element)\n",
    "\n",
    "    return private_idx,  torch.Tensor(public_idx).to(torch.int64), dct_block_reorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitive Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User 1\n",
    "overall_img_path_list = []\n",
    "path_prefix = \"/home/jianming/work/multiface/dataset/m--20180227--0000--6795937--GHS/unwrapped_uv_1024/\"\n",
    "all_dir = os.listdir(path_prefix)\n",
    "for sgl_dir in all_dir:\n",
    "    path_average = os.path.join(path_prefix + sgl_dir, \"average\")\n",
    "    overall_img_path_list.append(os.path.join(path_average, os.listdir(path_average)[0]))\n",
    "\n",
    "highest_frequency_components_list = []\n",
    "for img_path in overall_img_path_list:\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    x = transform(image).unsqueeze(0)\n",
    "    highest_frequency_components_list.append(x)\n",
    "\n",
    "# User 2\n",
    "overall_img_path_list2 = []\n",
    "path_prefix2 = \"/scratch1/jianming/multiface/dataset/m--20180226--0000--6674443--GHS/unwrapped_uv_1024/\"\n",
    "all_dir = os.listdir(path_prefix2)\n",
    "for sgl_dir in all_dir:\n",
    "    path_average2 = os.path.join(path_prefix2 + sgl_dir, \"average\")\n",
    "    overall_img_path_list2.append(os.path.join(path_average2, os.listdir(path_average2)[0]))\n",
    "\n",
    "highest_frequency_components_list2 = []\n",
    "for img_path in overall_img_path_list2:\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    x = transform(image).unsqueeze(0)\n",
    "    highest_frequency_components_list2.append(x)\n",
    "\n",
    "# Two users\n",
    "highest_frequency_components_overall = highest_frequency_components_list + highest_frequency_components_list2\n",
    "num_images = len(highest_frequency_components_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average L2 norm is 17.73265838623047 for noise 0.01\n",
      "average L2 norm is 88.67213439941406 for noise 0.05\n",
      "average L2 norm is 177.37962341308594 for noise 0.1\n",
      "average L2 norm is 354.73406982421875 for noise 0.2\n",
      "average L2 norm is 532.0896606445312 for noise 0.3\n",
      "average L2 norm is 709.4993286132812 for noise 0.4\n",
      "average L2 norm is 886.6399536132812 for noise 0.5\n",
      "average L2 norm is 1064.2862548828125 for noise 0.6\n",
      "average L2 norm is 1241.5096435546875 for noise 0.7\n",
      "average L2 norm is 1418.840576171875 for noise 0.8\n",
      "average L2 norm is 1596.6778564453125 for noise 0.9\n",
      "average L2 norm is 1774.0244140625 for noise 1\n"
     ]
    }
   ],
   "source": [
    "highest_frequency_components_overall_test = highest_frequency_components_overall[:10]\n",
    "\n",
    "isotropic_noise_covariance = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "for noise_value in isotropic_noise_covariance:\n",
    "    l2_norm_list = []\n",
    "    for i, img  in  enumerate(highest_frequency_components_overall_test):\n",
    "        img = img.squeeze(0)\n",
    "        noise_val = torch.Tensor(np.random.normal(0, noise_value, img.shape))\n",
    "        noisy_img = img + noise_val\n",
    "        # transforms.functional.to_pil_image(noisy_img).save(f'downsample_original_img_{i}_noise_{noise_value}.png')\n",
    "        l2_norm_list.append(np.linalg.norm(noisy_img - img))\n",
    "    avg_l2_norm = np.mean(l2_norm_list)\n",
    "    print(f\"average L2 norm is {avg_l2_norm} for noise {noise_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pica37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
